{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Profiling Packages\n",
    "import cProfile\n",
    "import pstats\n",
    "import snakeviz\n",
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/aidan.mcloughlin/Library/CloudStorage/OneDrive-Veracyte,Inc/Desktop/work/dmPC\n",
      "['/Users/aidan.mcloughlin/Library/CloudStorage/OneDrive-Veracyte,Inc/Desktop/work/dmPC/scripts_model/profiling', '/Users/aidan.mcloughlin/miniconda3/envs/drug_resp/lib/python3.9/site-packages/git/ext/gitdb', '/Users/aidan.mcloughlin/miniconda3/envs/drug_resp/lib/python39.zip', '/Users/aidan.mcloughlin/miniconda3/envs/drug_resp/lib/python3.9', '/Users/aidan.mcloughlin/miniconda3/envs/drug_resp/lib/python3.9/lib-dynload', '', '/Users/aidan.mcloughlin/miniconda3/envs/drug_resp/lib/python3.9/site-packages', '/Users/aidan.mcloughlin/Library/CloudStorage/OneDrive-Veracyte,Inc/Desktop/work/dmPC/scripts_model']\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "print(os.getcwd())\n",
    "sys.path.append(os.getcwd()+\"/scripts_model\")\n",
    "\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytorch version: 2.0.1\n",
      "orig num threads: 8\n",
      "Cancer type coding map: \n",
      "  C_type  code  count\n",
      "0   grp0     0     62\n",
      "4   grp1     1     35\n",
      "8   grp2     2     41\n",
      "Cancer type coding map: \n",
      "  C_type  code  count\n",
      "0   grp0     0     62\n",
      "3   grp1     1     76\n",
      "Training data: \n",
      "   cdr: (117, 30)\n",
      "   c_data: (117, 355) \n",
      "   d_data: (30, 150)\n",
      "   Number of each initial cancer clusters: \n",
      "\n",
      "code    0   1  Total\n",
      "code                \n",
      "0      54   0     54\n",
      "1       0  30     30\n",
      "2       0  33     33\n",
      "Total  54  63    117\n",
      "\n",
      "Testing data:  \n",
      "   cdr: (21, 30)\n",
      "   c_data: (21, 355) \n",
      "   d_data: (30, 150)\n",
      "   Number of each initial cancer clusters: \n",
      "\n",
      "code   0   1  Total\n",
      "code               \n",
      "0      8   0      8\n",
      "1      0   5      5\n",
      "2      0   8      8\n",
      "Total  8  13     21\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "## Setup code from 2_2 cluster simulation\n",
    "from setup_simu2_2 import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the brunt of run time is contained within each round of VAE+predictor tuning.  The run time is not noticeably changing between rounds.  We would not expect the run times of these to be so long given how quickly the VAEs are pretraining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Initialize C-VAE:\n",
      "        Best epoc with test loss: epoch 78\n",
      "        Running time: 0.37354612350463867\n",
      "=> Initialize D-VAE:\n",
      "        Best epoc with test loss: epoch 52\n",
      "        Running time: 0.1433250904083252\n",
      "########################################################\n",
      "#### 0. k = 0                                     \n",
      "########################################################\n",
      "  ===================================\n",
      "  === 0.1. Training local CDP model \n",
      "  ===================================\n",
      "     -- round 0 -------------\n",
      "       a. Training D_VAE and Predictor\n",
      "            Best epoc with test loss: epoch 5\n",
      "            Running time: 11.744307041168213\n",
      "       b. 10 sensitive drug(s)\n",
      "       c. Training C_VAE and Predictor\n",
      "            Best epoc with test loss: epoch 9\n",
      "            Running time: 8.398390054702759\n",
      "       d. 54 cancer cell line(s) in the cluster\n",
      "     -- round 1 -------------\n",
      "       a. Training D_VAE and Predictor\n",
      "            Best epoc with test loss: epoch 6\n",
      "            Running time: 11.73641324043274\n",
      "       b. 10 sensitive drug(s)\n",
      "       c. Training C_VAE and Predictor\n",
      "            Best epoc with test loss: epoch 11\n",
      "            Running time: 8.62075400352478\n",
      "       d. 54 cancer cell line(s) in the cluster\n",
      "     -- round 2 -------------\n",
      "       a. Training D_VAE and Predictor\n",
      "            Best epoc with test loss: epoch 9\n",
      "            Running time: 12.01839017868042\n",
      "       b. 10 sensitive drug(s)\n",
      "       c. Training C_VAE and Predictor\n",
      "            Best epoc with test loss: epoch 6\n",
      "            Running time: 8.253574848175049\n",
      "       d. 54 cancer cell line(s) in the cluster\n",
      "     -- round 3 -------------\n",
      "       a. Training D_VAE and Predictor\n",
      "            Best epoc with test loss: epoch 5\n",
      "            Running time: 11.823858261108398\n",
      "       b. 10 sensitive drug(s)\n",
      "       c. Training C_VAE and Predictor\n",
      "            Best epoc with test loss: epoch 7\n",
      "            Running time: 8.52363896369934\n",
      "       d. 54 cancer cell line(s) in the cluster\n",
      "     -- round 4 -------------\n",
      "       a. Training D_VAE and Predictor\n",
      "            Best epoc with test loss: epoch 5\n",
      "            Running time: 11.904958009719849\n",
      "       b. 10 sensitive drug(s)\n",
      "       c. Training C_VAE and Predictor\n",
      "            Best epoc with test loss: epoch 7\n",
      "            Running time: 8.680258989334106\n",
      "       d. 54 cancer cell line(s) in the cluster\n",
      "  ===================================\n",
      "  === 0.2. sub local CDP model      \n",
      "  ===================================\n",
      "     -- round 0 -------------\n",
      "       a. Training D_VAE and Predictor\n",
      "            Best epoc with test loss: epoch 6\n",
      "            Running time: 5.438443899154663\n",
      "       b. 0 sensitive drug(s)\n",
      "  No subcluster found\n",
      "########################################################\n",
      "#### 1. k = 1                                     \n",
      "########################################################\n",
      "  ===================================\n",
      "  === 1.1. Training local CDP model \n",
      "  ===================================\n",
      "     -- round 0 -------------\n",
      "       a. Training D_VAE and Predictor\n",
      "            Best epoc with test loss: epoch 31\n",
      "            Running time: 13.639219760894775\n",
      "       b. 12 sensitive drug(s)\n",
      "       c. Training C_VAE and Predictor\n",
      "            Best epoc with test loss: epoch 6\n",
      "            Running time: 10.196231365203857\n",
      "       d. 63 cancer cell line(s) in the cluster\n",
      "     -- round 1 -------------\n",
      "       a. Training D_VAE and Predictor\n",
      "            Best epoc with test loss: epoch 5\n",
      "            Running time: 13.382632970809937\n",
      "       b. 11 sensitive drug(s)\n",
      "       c. Training C_VAE and Predictor\n",
      "            Best epoc with test loss: epoch 12\n",
      "            Running time: 9.063719987869263\n",
      "       d. 63 cancer cell line(s) in the cluster\n",
      "     -- round 2 -------------\n",
      "       a. Training D_VAE and Predictor\n",
      "            Best epoc with test loss: epoch 5\n",
      "            Running time: 13.52717113494873\n",
      "       b. 10 sensitive drug(s)\n",
      "       c. Training C_VAE and Predictor\n",
      "            Best epoc with test loss: epoch 5\n",
      "            Running time: 8.288733005523682\n",
      "       d. 63 cancer cell line(s) in the cluster\n",
      "     -- round 3 -------------\n",
      "       a. Training D_VAE and Predictor\n",
      "            Best epoc with test loss: epoch 6\n",
      "            Running time: 13.558212041854858\n",
      "       b. 10 sensitive drug(s)\n",
      "       c. Training C_VAE and Predictor\n",
      "            Best epoc with test loss: epoch 7\n",
      "            Running time: 8.315350770950317\n",
      "       d. 63 cancer cell line(s) in the cluster\n",
      "     -- round 4 -------------\n",
      "       a. Training D_VAE and Predictor\n",
      "            Best epoc with test loss: epoch 5\n",
      "            Running time: 13.365187168121338\n",
      "       b. 12 sensitive drug(s)\n",
      "       c. Training C_VAE and Predictor\n",
      "            Best epoc with test loss: epoch 7\n",
      "            Running time: 10.342127799987793\n",
      "       d. 63 cancer cell line(s) in the cluster\n",
      "  ===================================\n",
      "  === 1.2. sub local CDP model      \n",
      "  ===================================\n",
      "     -- round 0 -------------\n",
      "       a. Training D_VAE and Predictor\n",
      "            Best epoc with test loss: epoch 8\n",
      "            Running time: 5.377964973449707\n",
      "       b. 8 sensitive drug(s)\n",
      "       c. Training C_VAE and Predictor\n",
      "            Best epoc with test loss: epoch 5\n",
      "            Running time: 6.962083101272583\n",
      "       d. 37 cancer cell line(s) in the cluster\n",
      "     -- round 1 -------------\n",
      "       a. Training D_VAE and Predictor\n",
      "            Best epoc with test loss: epoch 5\n",
      "            Running time: 5.082510709762573\n",
      "       b. 8 sensitive drug(s)\n",
      "       c. Training C_VAE and Predictor\n",
      "            Best epoc with test loss: epoch 8\n",
      "            Running time: 6.970300912857056\n",
      "       d. 30 cancer cell line(s) in the cluster\n",
      "     -- round 2 -------------\n",
      "       a. Training D_VAE and Predictor\n",
      "            Best epoc with test loss: epoch 6\n",
      "            Running time: 4.313921928405762\n",
      "       b. 8 sensitive drug(s)\n",
      "       c. Training C_VAE and Predictor\n",
      "            Best epoc with test loss: epoch 5\n",
      "            Running time: 6.748349189758301\n",
      "       d. 30 cancer cell line(s) in the cluster\n",
      "     -- round 3 -------------\n",
      "       a. Training D_VAE and Predictor\n",
      "            Best epoc with test loss: epoch 6\n",
      "            Running time: 4.161133050918579\n",
      "       b. 8 sensitive drug(s)\n",
      "       c. Training C_VAE and Predictor\n",
      "            Best epoc with test loss: epoch 8\n",
      "            Running time: 6.991703987121582\n",
      "       d. 30 cancer cell line(s) in the cluster\n",
      "     -- round 4 -------------\n",
      "       a. Training D_VAE and Predictor\n",
      "            Best epoc with test loss: epoch 5\n",
      "            Running time: 4.227515935897827\n",
      "       b. 8 sensitive drug(s)\n",
      "       c. Training C_VAE and Predictor\n",
      "            Best epoc with test loss: epoch 6\n",
      "            Running time: 7.140311002731323\n",
      "       d. 30 cancer cell line(s) in the cluster\n",
      "Subcluster found as cluster 2\n",
      "########################################################\n",
      "#### Check all subclusters                              \n",
      "########################################################\n",
      "update\n",
      " - Cluster 1 found a subcluster with cluster ID: 2\n",
      "   - Now we have 3 clusters\n"
     ]
    }
   ],
   "source": [
    "CDPmodel = CDPmodel(K, CDPmodel_args)\n",
    "n_rounds = 5\n",
    "fit_returns = CDPmodel.fit(c_train, c_meta_train, d_data, cdr_train, train_args, n_rounds=n_rounds, search_subcluster=True, device = device)\n",
    "c_meta, c_meta_hist, d_sens_hist, losses_train_hist_list, best_epos_list, C_VAE_init_losses, D_VAE_init_losses, c_latent_list, d_latent_list = fit_returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used VSCode debug mode to collect the environment state during the round 0 local model training functions are invoked.  We load them here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            0\n",
      "0                       model\n",
      "1                data_loaders\n",
      "2               c_names_k_old\n",
      "3               d_names_k_old\n",
      "4           C_VAE_loss_weight\n",
      "5         C_recon_loss_weight\n",
      "6                C_kld_weight\n",
      "7   C_cluster_distance_weight\n",
      "8       C_update_ratio_weight\n",
      "9           D_VAE_loss_weight\n",
      "10        D_recon_loss_weight\n",
      "11               D_kld_weight\n",
      "12  D_cluster_distance_weight\n",
      "13      D_update_ratio_weight\n",
      "14        predict_loss_weight\n",
      "15                sens_cutoff\n",
      "16                  optimizer\n",
      "17                   n_epochs\n",
      "18                  scheduler\n",
      "19                  save_path\n"
     ]
    }
   ],
   "source": [
    "with open(\"scripts_model/profiling/output/d_vae_model_dict.pkl\", 'rb') as f:\n",
    "    d_vae_model_dict = pkl.load(f)\n",
    "\n",
    "with open(\"scripts_model/profiling/output/c_vae_model_dict.pkl\", 'rb') as f:\n",
    "    c_vae_model_dict = pkl.load(f)\n",
    "\n",
    "print(pd.DataFrame(d_vae_model_dict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.30401863715783345\n",
      "0.096\n"
     ]
    }
   ],
   "source": [
    "## Data-Loading Times:\n",
    "print(1.044 / 3434 * 1000)\n",
    "print(0.048 / 500 * 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO list for profiling:\n",
    "* find an efficient re-write of d_overlap_ratio and c_overlap_ratio\n",
    "* there appear to be redundant batches. debug why this is happening.\n",
    "* pre-compute encodings in each round will speed up the model\n",
    "* compare the result if we set a very large batch size to the model.\n",
    "* I'll look into literature for gene importance scores with respect to VAE latent space (w/ clusters?).\n",
    "* Prototype learning for compounds? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setup is done.\n",
      "NUMBER OF BATCHES: 18\n",
      "BATCH SIZE: 50\n",
      "BATCH DIMENSIONS: \n",
      "torch.Size([50, 355])\n",
      "torch.Size([50, 150])\n",
      "NUMBER OF BATCHES: 5\n",
      "BATCH SIZE: 50\n",
      "BATCH DIMENSIONS: \n",
      "torch.Size([50, 355])\n",
      "torch.Size([50, 150])\n",
      "            Best epoc with test loss: epoch 7\n",
      "NUMBER OF CALL TO DRUG SENS CUTOFF FNCT: 0\n"
     ]
    }
   ],
   "source": [
    "c_vae_model_dict['sens_cutoff'] = 0.5\n",
    "train_cvae_w_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setup is done.\n",
      "NUMBER OF BATCHES: 25\n",
      "BATCH SIZE: 50\n",
      "BATCH DIMENSIONS: \n",
      "torch.Size([50, 355])\n",
      "torch.Size([50, 150])\n",
      "NUMBER OF BATCHES: 7\n",
      "BATCH SIZE: 50\n",
      "BATCH DIMENSIONS: \n",
      "torch.Size([50, 355])\n",
      "torch.Size([50, 150])\n",
      "            Best epoc with test loss: epoch 9\n",
      "NUMBER OF CALL TO DRUG SENS CUTOFF FNCT: 0\n"
     ]
    }
   ],
   "source": [
    "cProfile.run('train_dvae_w_args()', str(root) + \"/scripts_model/profiling/output/dvae_state.prof\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Nov 26 21:59:43 2023    /Users/aidan.mcloughlin/Library/CloudStorage/OneDrive-Veracyte,Inc/Desktop/work/dmPC/scripts_model/profiling/output/dvae_state.prof\n",
      "\n",
      "         5452206 function calls (5253774 primitive calls) in 10.216 seconds\n",
      "\n",
      "   Ordered by: cumulative time\n",
      "   List reduced from 341 to 75 due to restriction <75>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        1    0.000    0.000   10.216   10.216 {built-in method builtins.exec}\n",
      "        1    0.000    0.000   10.216   10.216 <string>:1(<module>)\n",
      "        1    0.000    0.000   10.216   10.216 35102567.py:317(train_dvae_w_args)\n",
      "        1    1.499    1.499   10.215   10.215 35102567.py:2(train_CDPmodel_local)\n",
      "153600/9600    0.121    0.000    2.800    0.000 module.py:1494(_call_impl)\n",
      "     3200    0.010    0.000    2.697    0.001 models.py:718(forward)\n",
      "     6400    0.011    0.000    2.409    0.000 models.py:823(forward)\n",
      "41600/28800    0.052    0.000    2.151    0.000 container.py:215(forward)\n",
      "     6400    0.016    0.000    1.899    0.000 models.py:784(encode_)\n",
      "     2500    0.003    0.000    1.770    0.001 _tensor.py:428(backward)\n",
      "     2500    0.006    0.000    1.766    0.001 __init__.py:106(backward)\n",
      "     2500    0.015    0.000    1.751    0.001 optimizer.py:265(wrapper)\n",
      "     2500    1.747    0.001    1.747    0.001 {method 'run_backward' of 'torch._C._EngineBase' objects}\n",
      "     2500    0.008    0.000    1.698    0.001 optimizer.py:29(_use_grad)\n",
      "     2500    0.007    0.000    1.685    0.001 adam.py:108(step)\n",
      "     2500    0.004    0.000    1.583    0.001 adam.py:231(adam)\n",
      "     2500    0.516    0.000    1.559    0.001 adam.py:300(_single_tensor_adam)\n",
      "     6400    0.025    0.000    1.504    0.000 batchnorm.py:137(forward)\n",
      "     6400    0.014    0.000    1.451    0.000 functional.py:2419(batch_norm)\n",
      "     6400    1.427    0.000    1.427    0.000 {built-in method torch.batch_norm}\n",
      "     3434    0.009    0.000    1.090    0.000 dataloader.py:628(__next__)\n",
      "     3434    0.132    0.000    1.044    0.000 dataloader.py:675(_next_data)\n",
      "     3232    0.004    0.000    0.877    0.000 fetch.py:46(fetch)\n",
      "     3232    0.022    0.000    0.571    0.000 fetch.py:51(<listcomp>)\n",
      "   156651    0.070    0.000    0.549    0.000 dataset.py:195(__getitem__)\n",
      "     3200    0.093    0.000    0.519    0.000 losses.py:67(cluster_mu_distance)\n",
      "   939906    0.479    0.000    0.479    0.000 dataset.py:196(<genexpr>)\n",
      "    44800    0.038    0.000    0.464    0.000 linear.py:113(forward)\n",
      "    44800    0.408    0.000    0.408    0.000 {built-in method torch._C._nn.linear}\n",
      "     3232    0.003    0.000    0.301    0.000 collate.py:204(default_collate)\n",
      "    65000    0.300    0.000    0.300    0.000 {method 'sqrt' of 'torch._C._TensorBase' objects}\n",
      "19392/3232    0.024    0.000    0.298    0.000 collate.py:87(collate)\n",
      "    77259    0.297    0.000    0.297    0.000 {built-in method torch.mean}\n",
      "   136400    0.278    0.000    0.278    0.000 {method 'mul_' of 'torch._C._TensorBase' objects}\n",
      "     6400    0.014    0.000    0.266    0.000 models.py:812(decode)\n",
      "     3200    0.009    0.000    0.260    0.000 models.py:891(forward)\n",
      "     3232    0.004    0.000    0.242    0.000 collate.py:142(<listcomp>)\n",
      "   144600    0.235    0.000    0.235    0.000 {method 'add_' of 'torch._C._TensorBase' objects}\n",
      "     6400    0.070    0.000    0.234    0.000 models.py:834(reparameterize)\n",
      "    16160    0.010    0.000    0.233    0.000 collate.py:153(collate_tensor_fn)\n",
      "     6400    0.161    0.000    0.232    0.000 {built-in method builtins.sum}\n",
      "    16160    0.223    0.000    0.223    0.000 {built-in method torch.stack}\n",
      "     3200    0.008    0.000    0.132    0.000 losses.py:17(custom_vae_loss)\n",
      "     6400    0.120    0.000    0.120    0.000 {built-in method torch.randn_like}\n",
      "     3200    0.063    0.000    0.109    0.000 optimizer.py:435(zero_grad)\n",
      "     6400    0.009    0.000    0.102    0.000 functional.py:1176(cdist)\n",
      "    65000    0.095    0.000    0.095    0.000 {method 'addcdiv_' of 'torch._C._TensorBase' objects}\n",
      "     2500    0.071    0.000    0.093    0.000 adam.py:66(_init_group)\n",
      "     6400    0.091    0.000    0.091    0.000 {built-in method torch.cdist}\n",
      "    65000    0.074    0.000    0.074    0.000 {method 'addcmul_' of 'torch._C._TensorBase' objects}\n",
      "     3200    0.003    0.000    0.071    0.000 _tensor.py:920(__iter__)\n",
      "    12800    0.006    0.000    0.070    0.000 activation.py:776(forward)\n",
      "   190600    0.067    0.000    0.069    0.000 module.py:1601(__getattr__)\n",
      "     3200    0.066    0.000    0.066    0.000 {method 'unbind' of 'torch._C._TensorBase' objects}\n",
      "    12800    0.007    0.000    0.064    0.000 functional.py:1618(leaky_relu)\n",
      "     9134    0.008    0.000    0.063    0.000 profiler.py:491(__enter__)\n",
      "     3200    0.062    0.000    0.062    0.000 35102567.py:199(<listcomp>)\n",
      "    65000    0.033    0.000    0.058    0.000 optimizer.py:39(_get_value)\n",
      "    12800    0.056    0.000    0.056    0.000 {built-in method torch._C._nn.leaky_relu}\n",
      "    16000    0.008    0.000    0.055    0.000 dropout.py:58(forward)\n",
      "     9134    0.003    0.000    0.055    0.000 _ops.py:497(__call__)\n",
      "     6400    0.006    0.000    0.053    0.000 loss.py:20(__init__)\n",
      "     9134    0.051    0.000    0.051    0.000 {built-in method torch._ops.profiler._record_function_enter_new}\n",
      "     3200    0.003    0.000    0.051    0.000 loss.py:618(forward)\n",
      "     6404    0.004    0.000    0.050    0.000 {built-in method numpy.core._multiarray_umath.implement_array_function}\n",
      "    16000    0.028    0.000    0.048    0.000 functional.py:1235(dropout)\n",
      "     9598    0.048    0.000    0.048    0.000 {method 'mean' of 'torch._C._TensorBase' objects}\n",
      "     3200    0.005    0.000    0.047    0.000 functional.py:3032(binary_cross_entropy)\n",
      "     3202    0.003    0.000    0.045    0.000 <__array_function__ internals>:177(unique)\n",
      "     6400    0.043    0.000    0.043    0.000 {built-in method torch.exp}\n",
      "    65000    0.027    0.000    0.041    0.000 optimizer.py:52(_dispatch_sqrt)\n",
      "     3200    0.002    0.000    0.040    0.000 loss.py:535(forward)\n",
      "     8232    0.012    0.000    0.040    0.000 {built-in method builtins.all}\n",
      "     3200    0.002    0.000    0.040    0.000 loss.py:615(__init__)\n",
      "     3200    0.039    0.000    0.039    0.000 {built-in method torch._C._nn.binary_cross_entropy}\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pstats.Stats at 0x14a1add30>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = pstats.Stats(str(root) + \"/scripts_model/profiling/output/dvae_state.prof\")\n",
    "p.strip_dirs().sort_stats(2).print_stats(75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAE pretrainer profiling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00038\n",
      "0.0007781250000000001\n"
     ]
    }
   ],
   "source": [
    "## Forward time comparisons (VAE vs Full Model):\n",
    "print(.114 / 300)\n",
    "print(2.490 / 3200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Best epoc with test loss: epoch 99\n",
      "        Running time: 0.4088881015777588\n",
      "Sun Nov 26 14:11:56 2023    /Users/aidan.mcloughlin/Library/CloudStorage/OneDrive-Veracyte,Inc/Desktop/work/dmPC/scripts_model/profiling/output/vae_pretrain.prof\n",
      "\n",
      "         307402 function calls (296061 primitive calls) in 0.410 seconds\n",
      "\n",
      "   Ordered by: cumulative time\n",
      "   List reduced from 525 to 50 due to restriction <50>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        1    0.000    0.000    0.410    0.410 {built-in method builtins.exec}\n",
      "        1    0.000    0.000    0.410    0.410 <string>:1(<module>)\n",
      "        1    0.000    0.000    0.410    0.410 2494660929.py:12(train_cvae_w_args)\n",
      "        1    0.000    0.000    0.410    0.410 trainers.py:748(train_VAE)\n",
      "        1    0.018    0.018    0.409    0.409 trainers.py:636(train_VAE_train)\n",
      " 4500/600    0.005    0.000    0.121    0.000 module.py:1494(_call_impl)\n",
      "      300    0.001    0.000    0.114    0.000 models.py:823(forward)\n",
      "      200    0.001    0.000    0.090    0.000 optimizer.py:265(wrapper)\n",
      " 1200/900    0.002    0.000    0.088    0.000 container.py:215(forward)\n",
      "      300    0.001    0.000    0.087    0.000 models.py:784(encode_)\n",
      "      200    0.001    0.000    0.085    0.000 optimizer.py:29(_use_grad)\n",
      "      200    0.001    0.000    0.084    0.000 adam.py:108(step)\n",
      "      200    0.000    0.000    0.079    0.000 adam.py:231(adam)\n",
      "      200    0.024    0.000    0.078    0.000 adam.py:300(_single_tensor_adam)\n",
      "      200    0.000    0.000    0.075    0.000 _tensor.py:428(backward)\n",
      "      200    0.001    0.000    0.075    0.000 __init__.py:106(backward)\n",
      "      200    0.073    0.000    0.073    0.000 {method 'run_backward' of 'torch._C._EngineBase' objects}\n",
      "      300    0.001    0.000    0.063    0.000 batchnorm.py:137(forward)\n",
      "      300    0.001    0.000    0.060    0.000 functional.py:2419(batch_norm)\n",
      "      300    0.058    0.000    0.058    0.000 {built-in method torch.batch_norm}\n",
      "      500    0.001    0.000    0.055    0.000 dataloader.py:628(__next__)\n",
      "      500    0.005    0.000    0.048    0.000 dataloader.py:675(_next_data)\n",
      "      300    0.000    0.000    0.036    0.000 fetch.py:46(fetch)\n",
      "      300    0.002    0.000    0.023    0.000 fetch.py:51(<listcomp>)\n",
      "     1500    0.002    0.000    0.022    0.000 linear.py:113(forward)\n",
      "    11700    0.004    0.000    0.021    0.000 dataset.py:195(__getitem__)\n",
      "     1500    0.020    0.000    0.020    0.000 {built-in method torch._C._nn.linear}\n",
      "    35100    0.017    0.000    0.017    0.000 dataset.py:196(<genexpr>)\n",
      "     2400    0.017    0.000    0.017    0.000 {method 'sqrt' of 'torch._C._TensorBase' objects}\n",
      "      300    0.001    0.000    0.016    0.000 losses.py:17(custom_vae_loss)\n",
      "      300    0.001    0.000    0.015    0.000 models.py:812(decode)\n",
      "     5400    0.014    0.000    0.014    0.000 {method 'mul_' of 'torch._C._TensorBase' objects}\n",
      "     5900    0.013    0.000    0.013    0.000 {method 'add_' of 'torch._C._TensorBase' objects}\n",
      "      300    0.000    0.000    0.013    0.000 collate.py:204(default_collate)\n",
      "  900/300    0.002    0.000    0.012    0.000 collate.py:87(collate)\n",
      "      300    0.004    0.000    0.012    0.000 models.py:834(reparameterize)\n",
      " 2800/200    0.002    0.000    0.011    0.000 module.py:2269(train)\n",
      "       10    0.000    0.000    0.009    0.001 serialization.py:388(save)\n",
      "      300    0.003    0.000    0.008    0.000 optimizer.py:435(zero_grad)\n",
      "      500    0.000    0.000    0.007    0.000 dataloader.py:622(_next_index)\n",
      "      801    0.000    0.000    0.007    0.000 {built-in method builtins.next}\n",
      "      300    0.000    0.000    0.007    0.000 collate.py:142(<listcomp>)\n",
      "      500    0.002    0.000    0.007    0.000 sampler.py:241(__iter__)\n",
      "     1000    0.001    0.000    0.007    0.000 profiler.py:491(__enter__)\n",
      "      600    0.001    0.000    0.007    0.000 collate.py:153(collate_tensor_fn)\n",
      "       10    0.003    0.000    0.007    0.001 serialization.py:592(_save)\n",
      "      300    0.000    0.000    0.007    0.000 loss.py:535(forward)\n",
      "      300    0.001    0.000    0.007    0.000 functional.py:3267(mse_loss)\n",
      "     3100    0.003    0.000    0.006    0.000 module.py:1617(__setattr__)\n",
      "      600    0.006    0.000    0.006    0.000 {built-in method torch.stack}\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pstats.Stats at 0x14854afd0>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"scripts_model/profiling/output/c_vae_pt_dict.pkl\", 'rb') as f:\n",
    "    c_vae_pt_dict = pkl.load(f)\n",
    "\n",
    "with open(\"scripts_model/profiling/output/d_vae_pt_dict.pkl\", 'rb') as f:\n",
    "    d_vae_pt_dict = pkl.load(f)\n",
    "#     \"vae_model\": self.CDPmodel_list[0].c_VAE, \n",
    "#     \"device\": device, \n",
    "#     \"c_data\": c_data, \n",
    "#     \"vae_type\": \"C\", \n",
    "#     \"save_path\": train_params['cVAE_save_path'], \n",
    "#     \"params\": train_params\n",
    "def train_cvae_w_args():\n",
    "    train_VAE(\n",
    "        c_vae_pt_dict['vae_model'], \n",
    "        c_vae_pt_dict['device'], \n",
    "        c_vae_pt_dict['c_data'], \n",
    "        c_vae_pt_dict['vae_type'], \n",
    "        c_vae_pt_dict['save_path'],\n",
    "        c_vae_pt_dict['params'])\n",
    "        \n",
    "\n",
    "## Profile:\n",
    "cProfile.run('train_cvae_w_args()', str(root) + \"/scripts_model/profiling/output/vae_pretrain.prof\")\n",
    "\n",
    "p = pstats.Stats(str(root) + \"/scripts_model/profiling/output/vae_pretrain.prof\")\n",
    "p.strip_dirs().sort_stats(2).print_stats(50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(c_vae_pt_dict['params'].batch_size)\n",
    "print(c_vae_pt_dict['c_data'].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The original local training function is below:\n",
    "def train_CDPmodel_local(\n",
    "        model, \n",
    "        data_loaders={}, \n",
    "        c_names_k_old=None, \n",
    "        d_names_k_old=None, \n",
    "        C_VAE_loss_weight = 1, \n",
    "        C_recon_loss_weight = 1, \n",
    "        C_kld_weight = None, \n",
    "        C_cluster_distance_weight=100, \n",
    "        C_update_ratio_weight = 100, \n",
    "        D_VAE_loss_weight = 1, \n",
    "        D_recon_loss_weight = 1, \n",
    "        D_kld_weight = None, \n",
    "        D_cluster_distance_weight=100, \n",
    "        D_update_ratio_weight = 100, \n",
    "        predict_loss_weight = 1, \n",
    "        sens_cutoff = 0.5,\n",
    "        optimizer=None, \n",
    "        n_epochs=100, \n",
    "        scheduler=None,\n",
    "        load=False, \n",
    "        save_path=\"model.pkl\",  \n",
    "        best_model_cache = \"drive\"):\n",
    "    \n",
    "    call_counter = 0\n",
    "\n",
    "    if(load!=False):\n",
    "        if(os.path.exists(save_path)):\n",
    "            model.load_state_dict(torch.load(save_path))           \n",
    "            return model, 0, 0\n",
    "        else:\n",
    "            logging.warning(\"Failed to load existing model file, proceed to the trainning process.\")\n",
    "    \n",
    "    dataset_sizes = {x: data_loaders[x].dataset.tensors[0].shape[0] for x in ['train', 'val']}\n",
    "    loss_hist = {}\n",
    "    c_vae_loss_hist = {}\n",
    "    c_recon_loss_hist = {}\n",
    "    c_kld_hist = {}\n",
    "    c_cluster_dist_hist = {}\n",
    "    c_update_overlap_hist = {}\n",
    "    d_vae_loss_hist = {}\n",
    "    d_recon_loss_hist = {}\n",
    "    d_kld_hist = {}\n",
    "    d_cluster_dist_hist = {}\n",
    "    d_update_overlap_hist = {}\n",
    "    prediction_loss_hist = {}\n",
    "\n",
    "    n_cells = len(np.unique(np.concatenate(\n",
    "        (data_loaders['train'].dataset.tensors[3].numpy(),\n",
    "        data_loaders['val'].dataset.tensors[3].numpy()))))\n",
    "    n_drugs = len(np.unique(np.concatenate(\n",
    "        (data_loaders['train'].dataset.tensors[4].numpy(),\n",
    "        data_loaders['val'].dataset.tensors[4].numpy()))))\n",
    "    \n",
    "    if best_model_cache == \"memory\":\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    else:\n",
    "        torch.save(model.state_dict(), save_path+\"_bestcahce.pkl\")\n",
    "    \n",
    "    best_loss = np.inf\n",
    "    best_epoch = -1\n",
    "\n",
    "    print(\"setup is done.\")\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        logging.info('Epoch {}/{}'.format(epoch, n_epochs - 1))\n",
    "        logging.info('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                # optimizer = scheduler(optimizer, epoch)\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()  # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_prediction_loss = 0.0\n",
    "            running_c_update_overlap = 0.0\n",
    "            running_d_update_overlap = 0.0\n",
    "\n",
    "            running_c_vae_loss = 0.0\n",
    "            running_c_recon_loss = 0.0\n",
    "            running_c_kld_loss = 0.0\n",
    "            running_c_cluster_d = 0.0\n",
    "            \n",
    "            running_d_vae_loss = 0.0\n",
    "            running_d_recon_loss = 0.0\n",
    "            running_d_kld_loss = 0.0\n",
    "            running_d_cluster_d = 0.0\n",
    "            \n",
    "\n",
    "            n_iters = len(data_loaders[phase])\n",
    "            batch_size = data_loaders[phase].batch_size\n",
    "\n",
    "\n",
    "            # Iterate over data.\n",
    "            # for data in data_loaders[phase]:\n",
    "            n_batches = 0\n",
    "            if epoch == 0:\n",
    "                for batchidx, (y, c_data, d_data, c_name, d_name) in enumerate(data_loaders[phase]):\n",
    "                    n_batches += 1\n",
    "            \n",
    "                print(\"NUMBER OF BATCHES: \" + str(n_batches))\n",
    "                print(\"BATCH SIZE: \" + str(data_loaders[phase].batch_size)) \n",
    "\n",
    "            for batchidx, (y, c_data, d_data, c_name, d_name) in enumerate(data_loaders[phase]):\n",
    "\n",
    "                if epoch == 0 and batchidx == 0:\n",
    "                    print(\"BATCH DIMENSIONS: \")\n",
    "                    print(c_data.shape)\n",
    "                    print(d_data.shape)\n",
    "\n",
    "                \n",
    "                # if c_data.shape[0] <= 2:\n",
    "                #     print(f'epoch: {epoch}, phase: {phase}, batch: {batchidx}')\n",
    "                #     print(f'c_data shape: {c_data.shape}')\n",
    "                #     print(f'd_data shape: {d_data.shape}')\n",
    "                \n",
    "                y.requires_grad_(True)\n",
    "                d_data.requires_grad_(True)\n",
    "\n",
    "                # encode and decode \n",
    "                c_mu, c_log_var, c_X_rec, d_mu, d_log_var, d_X_rec, y_hat = model(c_data, d_data)\n",
    "\n",
    "                sensitive = y_hat > sens_cutoff\n",
    "                sensitive = sensitive.long()\n",
    "\n",
    "                # compute loss\n",
    "\n",
    "                mse = nn.MSELoss(reduction=\"sum\")\n",
    "\n",
    "                #   1. Prediction loss: \n",
    "                bce = nn.BCELoss()\n",
    "                prediction_loss = bce(y_hat, y)\n",
    "                \n",
    "                if C_VAE_loss_weight > 0:\n",
    "                    # 2. C_VAE:\n",
    "                    # 2.1. VAE loss: reconstruction loss & kld\n",
    "                    C_recon_loss, C_kld = custom_vae_loss(c_data, c_mu, c_log_var, c_X_rec, mse)\n",
    "\n",
    "                    # 2.2. the loss of latent spaces distances:\n",
    "                    #     - distances of cells in the cluster to the cluster centroid \n",
    "                    #     + distances of cells outside the cluster to the cluster centroid \n",
    "                    C_latent_dist_loss = cluster_mu_distance(c_mu, sensitive)\n",
    "                    \n",
    "                    # adding all up\n",
    "                    if C_kld_weight is None:\n",
    "                        C_kld_weight = batch_size / dataset_sizes[phase]\n",
    "\n",
    "                    C_VAE_loss = C_recon_loss_weight * C_recon_loss + C_kld_weight * C_kld + C_cluster_distance_weight * C_latent_dist_loss # + C_update_ratio_weight * C_overlap_loss\n",
    "\n",
    "\n",
    "                    # 3. requiring the updated cluster overlaping with the old clustering\n",
    "                    c_name_dt = c_name.detach().numpy().astype(int)\n",
    "                    c_name_uq = np.sort(np.unique(c_name_dt))\n",
    "                    c_sens_k = torch.zeros(n_cells)\n",
    "                    for c in c_name_uq:\n",
    "                        c_sens_k[c] = torch.mean(y_hat[c_name_dt == c]) > sens_cutoff\n",
    "\n",
    "                    if len(set(c_names_k_old)) > 0:\n",
    "                        c_overlap_ratio = torch.sum(c_sens_k[c_names_k_old]) / sum([c in c_name_uq for c in c_names_k_old])\n",
    "                        C_overlap_loss = -c_overlap_ratio # at least 50% overlapping, the more overlap the better?\n",
    "                    else:\n",
    "                        C_overlap_loss = 0\n",
    "                \n",
    "                else:\n",
    "                    C_VAE_loss = 0\n",
    "                    C_overlap_loss = 0\n",
    "\n",
    "\n",
    "                \n",
    "                if D_VAE_loss_weight > 0:\n",
    "                    # 4. D_VAE\n",
    "                    # 4.1. VAE loss: reconstruction loss & kld\n",
    "                    D_recon_loss, D_kld = custom_vae_loss(d_data, d_mu, d_log_var, d_X_rec, mse)\n",
    "\n",
    "                    # 4.2. the loss of latent spaces distances:\n",
    "                    #     - distances of drugs in the cluster to the cluster centroid \n",
    "                    #     + distances of drugs outside the cluster to the cluster centroid \n",
    "                    D_latent_dist_loss = cluster_mu_distance(d_mu, sensitive)\n",
    "                    \n",
    "                    # adding all up\n",
    "                    if D_kld_weight is None:\n",
    "                        D_kld_weight = batch_size / dataset_sizes[phase]\n",
    "\n",
    "                    D_VAE_loss = D_recon_loss_weight * D_recon_loss + D_kld_weight * D_kld + D_cluster_distance_weight * D_latent_dist_loss # + D_update_ratio_weight * D_overlap_loss\n",
    "\n",
    "\n",
    "                    # 5 requiring the updated cluster overlaping with the old clustering\n",
    "                    d_name_dt = d_name.detach().numpy().astype(int)\n",
    "                    d_name_uq = np.sort(np.unique(d_name_dt))\n",
    "                    d_sens_k = torch.zeros(n_drugs)\n",
    "                    for d in d_name_uq:\n",
    "                        d_sens_k[d] = torch.mean(y_hat[d_name_dt == d]) > sens_cutoff\n",
    "\n",
    "                    if len(set(d_names_k_old)) > 0:\n",
    "                        d_overlap_ratio = torch.sum(d_sens_k[d_names_k_old]) / sum([d in d_name_uq for d in d_names_k_old])\n",
    "                        D_overlap_loss = - d_overlap_ratio # at least 50% overlapping, the more overlap the better?\n",
    "                    else:\n",
    "                        D_overlap_loss = 0\n",
    "                \n",
    "                else:\n",
    "                    D_VAE_loss = 0\n",
    "                    D_overlap_loss = 0\n",
    "\n",
    "                \n",
    "                # Add up three losses:\n",
    "                loss =  C_VAE_loss_weight * C_VAE_loss + D_VAE_loss_weight * D_VAE_loss + predict_loss_weight * prediction_loss + C_update_ratio_weight * C_overlap_loss + D_update_ratio_weight * D_overlap_loss\n",
    "                # if C_VAE_loss_weight > 0:\n",
    "                #     print(f'     C_VAE_loss: {C_VAE_loss}')\n",
    "                #     print(f\"   c: {C_VAE_loss_weight * C_VAE_loss}\")\n",
    "                #     print(f'     D_VAE_loss: {D_VAE_loss}')\n",
    "                #     print(f\"   d: {D_VAE_loss_weight * D_VAE_loss}\")\n",
    "                #     print(f'     prediction_loss: {prediction_loss}')\n",
    "                #     print(f\"   p: {predict_loss_weight * prediction_loss}\")\n",
    "                #     print(f'     C_overlap_loss: {C_overlap_loss}')\n",
    "                #     print(f\"   c_o: {C_update_ratio_weight * C_overlap_loss}\")\n",
    "                #     print(f'     D_overlap_loss: {D_overlap_loss}')\n",
    "                #     print(f\"   d_o: {D_update_ratio_weight * D_overlap_loss}\")\n",
    "                #     print(f'   Total loss: {loss}')\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # backward + optimize only if in training phase\n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    # addressing the instability by lowering the learning rate or use gradient clipping\n",
    "                    # torch.nn.utils.clip_grad_norm_(d_vae_predictor.parameters(), 0.01)\n",
    "                    # update the weights\n",
    "                    optimizer.step()\n",
    "\n",
    "                # print loss statistics\n",
    "                running_loss += loss.item()\n",
    "                running_prediction_loss += (predict_loss_weight * prediction_loss).item()   \n",
    "                running_c_update_overlap += (C_update_ratio_weight * C_overlap_loss)\n",
    "                running_d_update_overlap += (D_update_ratio_weight * D_overlap_loss)\n",
    "                # print(f\"   running_d_update_overlap: {running_d_update_overlap}\")\n",
    "\n",
    "                if C_VAE_loss_weight > 0:\n",
    "                    running_c_vae_loss += (C_VAE_loss_weight * C_VAE_loss).item()\n",
    "                    running_c_recon_loss += (C_VAE_loss_weight * C_recon_loss_weight * C_recon_loss).item()\n",
    "                    running_c_kld_loss += (C_VAE_loss_weight * C_kld_weight * C_kld).item()\n",
    "                    running_c_cluster_d += (C_VAE_loss_weight * C_cluster_distance_weight * C_latent_dist_loss).item()\n",
    "                    \n",
    "                if D_VAE_loss_weight > 0:            \n",
    "                    running_d_vae_loss += (D_VAE_loss_weight * D_VAE_loss).item()\n",
    "                    running_d_recon_loss += (D_VAE_loss_weight * D_recon_loss_weight * D_recon_loss).item()\n",
    "                    running_d_kld_loss += (D_VAE_loss_weight * D_kld_weight * D_kld).item()\n",
    "                    running_d_cluster_d += (D_VAE_loss_weight * D_cluster_distance_weight * D_latent_dist_loss).item()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_prediction_loss = running_prediction_loss / dataset_sizes[phase]\n",
    "            epoch_c_update_overlap = running_c_update_overlap / (batchidx + 1)\n",
    "            epoch_d_update_overlap = running_d_update_overlap / (batchidx + 1)\n",
    "\n",
    "            epoch_c_vae_loss = running_c_vae_loss / dataset_sizes[phase]\n",
    "            epoch_c_recon_loss = running_c_recon_loss / dataset_sizes[phase]\n",
    "            epoch_c_kld_loss = running_c_kld_loss / dataset_sizes[phase]\n",
    "            epoch_c_cluster_d = running_c_cluster_d / dataset_sizes[phase]\n",
    "\n",
    "            epoch_d_vae_loss = running_d_vae_loss / dataset_sizes[phase]\n",
    "            epoch_d_recon_loss = running_d_recon_loss / dataset_sizes[phase]\n",
    "            epoch_d_kld_loss = running_d_kld_loss / dataset_sizes[phase]\n",
    "            epoch_d_cluster_d = running_d_cluster_d / dataset_sizes[phase]\n",
    "            \n",
    "            #if phase == 'train':\n",
    "            #    scheduler.step(epoch_loss)\n",
    "                \n",
    "            # last_lr = scheduler.optimizer.param_groups[0]['lr']\n",
    "            loss_hist[epoch,phase] = epoch_loss\n",
    "            c_vae_loss_hist[epoch,phase] = epoch_c_vae_loss\n",
    "            c_recon_loss_hist[epoch,phase] = epoch_c_recon_loss\n",
    "            c_kld_hist[epoch,phase] = epoch_c_kld_loss\n",
    "            c_cluster_dist_hist[epoch,phase] = epoch_c_cluster_d\n",
    "            c_update_overlap_hist[epoch,phase] = epoch_c_update_overlap\n",
    "            d_vae_loss_hist[epoch,phase] = epoch_d_vae_loss\n",
    "            d_recon_loss_hist[epoch,phase] = epoch_d_recon_loss\n",
    "            d_kld_hist[epoch,phase] = epoch_d_kld_loss\n",
    "            d_cluster_dist_hist[epoch,phase] = epoch_d_cluster_d\n",
    "            d_update_overlap_hist[epoch,phase] = epoch_d_update_overlap\n",
    "            prediction_loss_hist[epoch,phase] = epoch_prediction_loss\n",
    "            \n",
    "            if phase == 'val' and epoch >= 5 and epoch_loss < best_loss:\n",
    "                best_loss = epoch_loss\n",
    "                best_epoch = epoch\n",
    "\n",
    "                if best_model_cache == \"memory\":\n",
    "                    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                else:\n",
    "                    torch.save(model.state_dict(), save_path+\"_bestcahce.pkl\")\n",
    "            elif phase == 'val' and best_loss == np.inf and epoch == round(n_epochs/2):\n",
    "                if best_model_cache == \"memory\":\n",
    "                    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                else:\n",
    "                    print(f'          save model half way (epoch {epoch}) since testing loss is NaN')\n",
    "                    torch.save(model.state_dict(), save_path+\"_bestcahce.pkl\")\n",
    "                    best_epoch = epoch\n",
    "                \n",
    "    train_hist = [loss_hist, c_vae_loss_hist, c_recon_loss_hist, c_kld_hist, c_cluster_dist_hist, c_update_overlap_hist, d_vae_loss_hist, d_recon_loss_hist, d_kld_hist, d_cluster_dist_hist, d_update_overlap_hist, prediction_loss_hist]\n",
    "\n",
    "    # Select best model wts if use memory to cahce models\n",
    "    if best_model_cache == \"memory\":\n",
    "        torch.save(best_model_wts, save_path)\n",
    "        model.load_state_dict(best_model_wts)  \n",
    "    else:\n",
    "        print(f'            Best epoc with test loss: epoch {best_epoch}')\n",
    "        model.load_state_dict((torch.load(save_path+\"_bestcahce.pkl\")))\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "\n",
    "\n",
    "    print('NUMBER OF CALL TO DRUG SENS CUTOFF FNCT: ' + str(call_counter))\n",
    "\n",
    "    return model, train_hist, best_epoch\n",
    "\n",
    "def train_dvae_w_args():\n",
    "    train_CDPmodel_local(\n",
    "        model = d_vae_model_dict['model'], \n",
    "        data_loaders = d_vae_model_dict['data_loaders'], \n",
    "        c_names_k_old = d_vae_model_dict['c_names_k_old'], \n",
    "        d_names_k_old = d_vae_model_dict['d_names_k_old'], \n",
    "        C_VAE_loss_weight = d_vae_model_dict['C_VAE_loss_weight'], \n",
    "        C_recon_loss_weight = d_vae_model_dict['C_recon_loss_weight'], \n",
    "        C_kld_weight = d_vae_model_dict['C_kld_weight'],\n",
    "        C_cluster_distance_weight = d_vae_model_dict['C_cluster_distance_weight'],\n",
    "        C_update_ratio_weight = d_vae_model_dict['C_update_ratio_weight'],\n",
    "        D_VAE_loss_weight = d_vae_model_dict['D_VAE_loss_weight'], \n",
    "        D_recon_loss_weight = d_vae_model_dict['D_recon_loss_weight'],  \n",
    "        D_kld_weight = d_vae_model_dict['D_kld_weight'],  \n",
    "        D_cluster_distance_weight = d_vae_model_dict['D_cluster_distance_weight'],  \n",
    "        D_update_ratio_weight = d_vae_model_dict['D_update_ratio_weight'],\n",
    "        predict_loss_weight = d_vae_model_dict['predict_loss_weight'],\n",
    "        sens_cutoff = d_vae_model_dict['sens_cutoff'],\n",
    "        optimizer = d_vae_model_dict['optimizer'], \n",
    "        n_epochs = d_vae_model_dict['n_epochs'], \n",
    "        scheduler = d_vae_model_dict['scheduler'],\n",
    "        save_path = d_vae_model_dict['save_path']\n",
    "    )    \n",
    "\n",
    "\n",
    "def train_cvae_w_args():\n",
    "    train_CDPmodel_local(\n",
    "        model = c_vae_model_dict['model'], \n",
    "        data_loaders = c_vae_model_dict['data_loaders'], \n",
    "        c_names_k_old = c_vae_model_dict['c_names_k_old'], \n",
    "        d_names_k_old = c_vae_model_dict['d_names_k_old'], \n",
    "        C_VAE_loss_weight = c_vae_model_dict['C_VAE_loss_weight'], \n",
    "        C_recon_loss_weight = c_vae_model_dict['C_recon_loss_weight'], \n",
    "        C_kld_weight = c_vae_model_dict['C_kld_weight'],\n",
    "        C_cluster_distance_weight = c_vae_model_dict['C_cluster_distance_weight'],\n",
    "        C_update_ratio_weight = c_vae_model_dict['C_update_ratio_weight'],\n",
    "        D_VAE_loss_weight = c_vae_model_dict['D_VAE_loss_weight'], \n",
    "        D_recon_loss_weight = c_vae_model_dict['D_recon_loss_weight'],  \n",
    "        D_kld_weight = c_vae_model_dict['D_kld_weight'],  \n",
    "        D_cluster_distance_weight = c_vae_model_dict['D_cluster_distance_weight'],  \n",
    "        D_update_ratio_weight = c_vae_model_dict['D_update_ratio_weight'],\n",
    "        predict_loss_weight = c_vae_model_dict['predict_loss_weight'],\n",
    "        sens_cutoff = c_vae_model_dict['sens_cutoff'],\n",
    "        optimizer = c_vae_model_dict['optimizer'], \n",
    "        n_epochs = c_vae_model_dict['n_epochs'], \n",
    "        scheduler = c_vae_model_dict['scheduler'],\n",
    "        save_path = c_vae_model_dict['save_path']\n",
    "    )    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(117, 355)\n",
      "(30, 150)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "66.96"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Confirm data loader has redundancies:\n",
    "print(c_train.shape)\n",
    "print(d_data.shape)\n",
    "\n",
    "np.sum(~np.isnan(cdr_train.to_numpy())) / 50 \n",
    "## doesn't explain the total number of batches. in fact we would still only be iterating through the active members of a bicluster."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drug_resp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
